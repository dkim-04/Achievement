
# A path to machine-learning

## summarization

## Introduction 
- 아직 MACHINE Learning(ML)시스템은 인간과 동물의 학습 능력과 세상에 대한 이해를 뛰어 넘지 못함

***AI가 이를 뛰어넘기 위해 해결해야 하는 3가지 과제***

1) 어떻게 기계는 관찰을 통해 세상을 표현하고, 예측하고 행동할 수 있는가?
   1) 세상에 대한 모델 학습: 기계는 관찰을 통해 세상의 작동 방식을 학습하는 예측적 세계 모델(Predictive World Model)을 개발할 수 있습니다. 이 모델은 현재 상태를 기반으로 미래의 상태를 예측하며, 이를 통      해 기계가 어떤 행동이 어떤 결과를 초래할지 이해할 수 있게 됩니다. 

    2) 계층적 표현 학습: 기계는 다양한 추상화 수준에서 세계를 표현할 수 있어야 한다. 예를 들어, 인간이 주변 환경을 이해할 때처럼 기계도 낮은 수준의 세부 정보(색상, 모양)부터 높은 수준의 개념(사물, 사건)      에 이르기까지 다양한 레벨에서 정보를 조직하고 학습할 수 있다. 이를 통해 기계는 더 복잡한 추론과 계획을 할 수 있다.

    3) 자기 지도 학습(Self-Supervised Learning): 기계는 최소한의 상호작용을 통해 세상의 모델을 학습해야 한다. 이를 위해 많은 양의 데이터를 스스로 분석하고 패턴을 발견하여 학습하는 자기 지도 학습 방법이       사용되어,외부의 명시적인 감독 없이도 기계가 세계의 작동 방식을 이해하고, 그에 따라 행동할 수 있도록 한다.

    4) 관찰 기반 예측과 행동: 기계는 관찰을 통해 축적한 세계에 대한 지식을 바탕으로 새로운 상황에서도 적절한 행동을 취할 수 있어야 한다. 관찰한 데이터와 내부 세계 모델을 바탕으로 미래 상황을 예측하고, 예      측된 결과에 따라 행동을 계획한다.

2) 경사 기반 학습과 호환되는 방식으로 기계가 추론하고 계획할 수 있는 방법은 무엇이 있을까?
- 기계가 경사 기반 학습을 활용하여 논리적이고 계획적인 행동을 할 수 있도록 하려면, 추론 과정이 기울기를 계산할 수 있는 미분 가능한 형태로 구성되어야 한다

3) 어떻게 기계는 다양한 추상화 수준과 다양한 시간 척도에서 계층적 방식으로 인식과 행동 계획을 표현하는 것을 배울 수 있을까?
-????



## 3.자율 지능을 위한 모델 구조
1) **구성 모듈**
 - 다른 모든 모듈로부터 입력을 받아 매개변수와 주의 회로를 변조하여 당면한 작업에 맞게 구성
2) **인식 모듈**
   - 센서로부터 신호를 받아 세상의 현재 상태를 추정
   - 주어진 작업에 대해 인식된 세계 상태의 작은 하위 집합만이 관련성이 있고 유용함
   - 여러 수준의 추상화를 통해 계층적 방식으로 세계의 상태를 나타낼 수 있음
3) **월드 모델 모듈**
   - 모듈아키텍처의 가장 복잡한 부분을 구성
   - 역할
     - 1) 인식을 통해 제공됮 않는 세계 상태에 대한 누락된 정보 추정
     - 2) 세계의 그럴듯한 미래 상태 예측   

* 월드 모델은 세계 관련 측면 일종의 시뮬레이터이다
* 구성자는 당면한 상황을 처리하기 위해 월드 모델을 구성
* 예측은 현재 작업과 관련된 정보를 포함하는 추상 표현 공간 내에서 수행
* 이상적으로, 월드 모델은 여러 수준의 추상화에서 세계 상태의 표현을 조작해 여러 시간 규모에 걸쳐 예측 가능

***모델 구조를 구축할 때 필요한 점***
   1)세계 모델이 여러 그럴듯한 예측을 하고 예측의 불확실성을 표현하도록 허용하는 방법 
   2) 세계 모델을 훈련하는 방법
   
4) **비용 모듈**
   - 모듈에이전트의 "불편함" 수준을 스칼라 양의 형태로 측정
   -  기계의 행동을 유도하는 중심적인 역할을 함
   -   내재적 비용과 학습 가능한 비용을 결합하여 기계는 비용을 최소화하는 방향으로 행동을 최적화
*주요 구성요소*
- 내제적 비용 모듈
- 비평가 또는 학습 가능한 비용

5) **내재 비용 모듈**
   - 에이전트의 궁극적인 목표는 장기적으로 내재 비용을 최소화하는 것
   - 내재 비용 모듈의 설계에 따라 에이전트 행동의 성격이 결정됨
     
 
6) **비평가 모듈**
   - 고유 에너지의 추정치를 예측
   - 훈련을 위해 비평가는 연관 메모리 모듈에 저장된 과거 상태와 후속 고유 비용을 검색하고 전자로부터 후자를 예측하도록 스스로 훈련
   - 비평가 모듈의 기능은 더 큰 작업의 일부로 특정 하위 목표를 향해 시스템을 지시하도록 구성기에 의해 동적으로 구성될 수 있다

7) **단기 기억 모듈**
   - 세계의 과거, 현재, 미래 상태에 대한 관련 정보와 그에 상응하는 내재 비용 가치를 저장
   - 이 모듈은 척추동물의 해마와 동일한 역할을 수행
   - 월드 모델은 단기 기억에 쿼리를 보내고 검색된 값을 받거나 새로운 상태 값을 저장할 수 있다.
  
8) **액터 모듈**
   - 행동 시퀸스 제안
   - 최적의 행동 시퀸스 선택
   - 결과적인 행동 실행
     * **구성요소**
     * 정책 모듈: 주어진 상태에 대해 즉각적인 행동 제안
     * 행동 최적화기: 예측된 비용을 최소화하는 최적의 행동 시퀸스를 찾아냄

### 일반적인 인식-행동 루프
- 기계의 행동 방식에 따라 두 가지 모드로 나뉨

1) **MODE-1:Reactive Behavior(즉각 반응형 행동)**
   - 기계가 간단한 방식으로 환경에 반응
   - 복잡한 계획이나 예측 없이 현재 인식된 상태에 따라 바로 행동을 결정

   **과정**
   1) 인식: 인식 모듈이 현재 세계 상태 인식 후 상태s[0] 생성
   2) 행동: actor 모듈의 정채 모듈이 상태에 따라 즉각적인 행동 결정
   3) 결과: 결정된 행동이 실행됨


2) **MODE-2:Reasoning and Planning using the World Model (세계 모델을 사용한 추론 및 계획)**


   **과정**
   1) 인식: 인식 모듈이 인식 후 상태로 표현
   2) 행동 제안: actor 모듈이 초기 행동 시퀸스 제안
   3) 시뮬레이션: 월드 모델이 제안된 행동 시퀸스 따라 미래 세계 상태 예측
   4) 평가: 비용 모듈이 예측된 세계 시퀸스에 총 비용 계산
   5) 계획: 액터 모듈이 비용을 최소화하는 방향으로 시퀸스 조정
   6) 행동 실행
   7) 메모리: 실행된 행동과 결과가 메모리에 저장되어 이후 학습 사용

#### 모드-2에서 모드-1로: 새로운 기술 학습

1) **Mode-2로 새로운 기술 학습**
   - Mode-2는 기계가 새로운 기술을 처음 학습할 때 사용
   - 이 모드에서는 기계가 월드 모델을 활용하여 시뮬레이션을 통해 여러 가지 행동 시퀀스를 평가하고, 그 중 가장 적합한 행동을 선택
   - 적의 행동 시퀀스를 찾기 위해 반복적인 시뮬레이션과 평가를 거친다

2) **Mode-1로 기술 전환**
   - Mode-2에서 학습된 결과를 기반으로, 기계는 더 간단하고 빠른 Mode-1 정책을 학습
   - 이 과정은 정책 모듈을 통해 이루어지며, Mode-2에서 얻은 최적의 행동 시퀀스와 실제 행동을 비교하여 정책 모듈의 파라미터를 조정     ## 파라미터란 : 여기에선 모델이나 시스템의 특정 특성을 나타내느 고정 값을 의미한다.


#### 행동 유도의 중심:비용모델
1) 비용 함수:
   - 총 비용은 내재적 비용과 학습 가능한 비용의 합으로 계산됨

2) 행동 유도:

   -비용 모듈은 기계의 행동을 유도합니다. 기계는 비용을 최소화하는 방향으로 행동을 결정하며, 이는 본능적인 욕구를 충족시키거나, 주어진 목표를 달성하는 방식으로 나타난다.

3) 비용 모듈의 학습:
   -비평가 모듈은 과거의 상태와 그로 인한 내재적 비용 데이터를 학습하여 미래의 비용을 예측할 수 있다. 이 예측은 기계가 복잡한 상황에서도 적절한 행동을 취할 수 있게 도와준다.
4) 행동 우선순위:
   - 구성자(Configurator) 모듈이 비용 모듈의 가중치를 조정하여 기계가 특정 하위 목표에 집중하도록 할 수 있다. 이를 통해 기계는 상황에 맞게 행동의 우선순위를 조정하고, 목표를 달성하는 데 필요한 동작을 취하게 된다
  

### 4.월드 모델 설계 및 훈련
- 기계가 세상의 작동 방식을 이해하고, 예측하고, 게획할 수 있는 모델을 설계하고 훈련하는 방법

#### 월드 모델 설계 및 훈련의 주요 과제
1) 다양한 상태 시퀸스 학습

2) 예측 불확실성 처리:
   -잠재 변수 사용해 복수의 예측 가능한 결과 표현

3) 다양한 시간 및 추상화 수준에서의 예측


#### 훈련 방법
1) 자기 주도 학습(SSL)
   -관찰한 데이터 기반으로 패턴을 완성하거나 미래 예측하는 작업 통해 학습
2) 잠재 변수와 불확실성
   - 잠재 변수는 주어진 현재 상태에서 예측할 수 없는 정보를 표현하며, 이를 통해 세계 모델이 여러 가능한 미래 상태를 생성할 수 있게 한다
3) 다중 모드 예측

4) 비교 및 평가
   - 세계 모델은 각 시나리오에 대해 가능한 결과를 시뮬레이션하고 평가
  


#### 자기 주도 학습(SSL)
 - 에너지 기반 모델(energy-Based Models, EBM)
    - 에너지는 주어진 입력(x)과 예측된 출력(y)의 일관성을 나타내는 값으로, 낮은 에너지는 두 데이터 간의 높은 일관성을 의미
 -  모델은 이 에너지를 최소화하여 데이터 간의 종속성을 학습한다.

#### 공동 임베딩 예측 구조(JEPA): joint Embedding predictive Architecture
 - 세계 상태를 표현하고 예측하는 방법을 학습하는 것을 목표로 함
 -  자기 지도 학습(SSL) 패러다임을 통해 예측적 세계 모델을 학습하는 것에 중점을 둠
 -  현재 상태와 미래 상태를 공유된 임베딩 공간에 매핑하여 간접적으로 예측을 수행

##### JEPA의 주요 개념
1) 공유 임베딩 공간
 - EPA에서는 현재 상태(입력)와 예측된 상태(출력)가 별도의 인코더를 통해 공유된 임베딩 공간에 매핑
 - 임베딩 공간은 입력과 출력의 중요한 정보를 추상화하여 표현하며, 이를 바탕으로 미래의 상태를 예측
2) 예측을 위한 잠재 변수(latenet variable) 사용
 -  이 잠재 변수는 현재 상태에서 직접 예측할 수 없는 정보를 포함하며, 다양한 잠재 변수를 통해 예측할 수 있는 가능한 미래 상태를 나타냄

##### JEPA 훈련방법

1) 비대조적 학습 방법
 - 특정 입력과 관련된 데이터를 다른 데이터와 비교하지 않고도 학습하는 방식
 - 모델은 주어진 데이터 내에서만 의미 있는 패턴이나 표현을 찾도록 훈련
 - 이를 통해 모델은 데이터의 구조를 이해하고, 새로운 데이터에서도 잘 일반화할 수 있는 능력을 갖추게 됨
 - 비대조적 학습은 예측 오차를 최소화하고, 잠재 변수의 정보량을 제한함으로써 모델이 학습된 샘플 이외의 지역에서도 적절한 에너지를 할당하도록 만듬

**훈련의 주요 요소**
1. sx와 sy의 정보 극대화
 - sx는 입력 데이터 x로부터 추출된 표현이고, sy는 출력 데이터 y로부터 추출된 표현으로, 각각 x,y에 최대한 많은 정보를 담도록 훈련됨

2. 예측 가능성 극대화
 - sx로부터 sy를 예측하는 것이 용이하도록 훈련, 이는 예측 과정에서 사용하는 에너지 함수D를 최소화하는 방향으로 모델 학습시킴
3. 잠재 변수 z의 정보 최소화
 - 잠재 변수 z는 sy를 예측하는 데 사용되는 추가적인 정보로 이를 최소화함으로써 불필요한 정보 포함 시키지 않도록 훈련


##### H-JEPA(Hierarchical JEPA) - 계층적 JEPA
1) 계층적 구조 
 - 여러 계층으로 구성되어 있으며, 각 계층은 데이터의 다른 수준의 추상화를 담당
 - 하위 계층은 세부적인 정보 처리, 상위 계층은 추상적인 정보 처리
 - 복잡한 데이터의 다양한 양상 효율적으로 학습 가능
 - 하위 게층에서는 (저수준 특징 학습), 상위 계층에서 이를 통합해 고수준의 개념 형성
2) 계층적 예측
 - 각 계층에서 데이터를 임베딩 공간에 매핑하고, 그 임베딩을 사용해 다음 계층의 예측 수행( 이 과정은 각 계층이 예측을 위한 기본 정보를 상위 계층에 전달하는 방식으로 이루어짐)
 - 에측은 저수준에서 고수준으로 진행, 저수준 계층의 예측 결과는 상위 계층의 예측에 영향 미침
3) 잠재 변수 사용

4) 추상화 수준에 따른 예측
 - 각 계층은 다른 시간적 또는 공간적 스케일에서 예측 수행
 - 저수준 계층은 짧은 시간 내에 일어나는 변화 예측
 - 고수준 계층은 장기적인 변화나 더 높은 수준의 개념 예측
 - 이러한 구조는 복잡한 시나리오에서 장기적인 예측을 수행하는데 유리

**장점**
* 복잡한 데이터 처리 유용함
* 다양한 예측 가능성 수행
* 효율적인 학습

##### 불확실성 처리
- 잠재 변수 활용
   * 확실한 예측을 다루기 위해 잠재 변수를 사용하여 다양한 예측을 생성
   * 각 잠재 변수는 예측에서 도출할 수 없는 정보를 포함하며, 이를 통해 다양한 가능한 미래 상태를 탐색할 수 있음
- 정규화
   *잠재 변수는 정규화되어야 에너지 붕괴를 방지하고, 시스템이 가능하면 잠재 변수의 도움 없이 최대한 예측할 수 있도록 강제함
- 샘플링
   * 샘플링을 통해 각 샘플은 다른 예측을 생성하며, 이러한 샘플링 과정을 통해 일관된 잠재 변수 시퀸스를 생성 가능
- 경로 탐색 및 가지치기
   * 잠재 변수의 샘플링으로 생성되는 경로의 수가 지수적으로 증가할 수 있기 때문에 경로 탐색 및 가지치기 전략을 사용해 이를 관리
 

##### 데이터 스트림
- 에이전트가 세상이 어떻게 작동하는지 배울 수 있는 다섯 가지 정보 수집 모드
1) **수동적 관찰**: 에이전트에 센서 스트림(예: 비디오, 오디오 등)이 공급
2) **active foveation**: 에이전트는 환경에 영향을 주지 않고 주의 집중이 이루어질 수 있는 흐름을 공급받고 있음
3) **passive agency(수동적)**: 환경에 작용하는 다른 에이전트가 관찰되는 감각 흐름을 통해 환경 상태에 대한 에이전트 작업의 인과 효과를 추론할 수 있음
4) **적극적인 자아감정**: 에이전트는 환경에 큰 영향을 주지 않고 센서의 위치를 수정할 수 있는 실제 또는 가상 환경에서 감각 스트림을 수신
5) **active agency**:에이전트의 행동에 의해 영향을 받는 감각 흐름. 이를 통해 에이전트가 자신의 행동의 결과를 예측하는 방법을 학습할 수 있는 인과 모델을 구축 가능


#### 5.액터 모델 디자인 및 훈련
1. 액터의 역할
   * 강화 학습에서 에이전트가 주어진 상태(state)에서 최적의 행동(action)을 선택하도록 하는 핵심 모듈로, 상태를 입력으로 받아 행동의 확률 분포를 출력하는 정책을 학습
2. 액터 설계
   * 정책 네크워크: 상태 공간을 입력 받아 가능한 행동의 확률 출력
   * 연속적 행동 구간: 행동 공간이 연속적이라면, 행동의 확률 분포를 파라미터화하고, 샘플링된 행동 선택 가능
   * 이산적 행동 구간: 행동이 이산적인 경우, Actor는 각 행동에 대한 확률을 출력하는 소프트맥스 함수를 사용하여 행동을 선택 가능
3. 액터 모델 훈련
   * 정책 그래디언트 방법: 태와 행동 사이의 매핑을 직접 학습한다. 보상을 최대화하기 위해 정책의 파라미터를 업데이트하는 방법임
   * 손실 함수: 보상을 예측하는 Critic의 출력을 사용해 손실 함수를 정의 가능. 에이전트가 얼마나 잘 행동했는지 평가
   * 모델 기반 강화 학습: 시뮬레이션된 환경에서 최적의 행동을 선택할 수 있도록 학습한다.이를 통해 실제 환경에서의 시행착오를 줄일 수 있다
4. 비판 모듈-액터 모듈
   * Critic은 주어진 상태와 행동 쌍의 가치를 평가하고, Actor는 이 평가를 바탕으로 정책을 개선한다. 이 방법은 Actor가 더 빠르게 그리고 안정적으로 학습할 수 있게 해준다
   
   
